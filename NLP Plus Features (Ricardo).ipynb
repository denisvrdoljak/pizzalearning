{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 5671 samples.\n",
      "Available attributes:  [u'giver_username_if_known', u'in_test_set', u'number_of_downvotes_of_request_at_retrieval', u'number_of_upvotes_of_request_at_retrieval', u'post_was_edited', u'request_id', u'request_number_of_comments_at_retrieval', u'request_text', u'request_text_edit_aware', u'request_title', u'requester_account_age_in_days_at_request', u'requester_account_age_in_days_at_retrieval', u'requester_days_since_first_post_on_raop_at_request', u'requester_days_since_first_post_on_raop_at_retrieval', u'requester_number_of_comments_at_request', u'requester_number_of_comments_at_retrieval', u'requester_number_of_comments_in_raop_at_request', u'requester_number_of_comments_in_raop_at_retrieval', u'requester_number_of_posts_at_request', u'requester_number_of_posts_at_retrieval', u'requester_number_of_posts_on_raop_at_request', u'requester_number_of_posts_on_raop_at_retrieval', u'requester_number_of_subreddits_at_request', u'requester_received_pizza', u'requester_subreddits_at_request', u'requester_upvotes_minus_downvotes_at_request', u'requester_upvotes_minus_downvotes_at_retrieval', u'requester_upvotes_plus_downvotes_at_request', u'requester_upvotes_plus_downvotes_at_retrieval', u'requester_user_flair', u'requester_username', u'unix_timestamp_of_request', u'unix_timestamp_of_request_utc']\n",
      "First post:\n",
      "{\n",
      "  \"giver_username_if_known\": \"N/A\", \n",
      "  \"in_test_set\": false, \n",
      "  \"number_of_downvotes_of_request_at_retrieval\": 2, \n",
      "  \"number_of_upvotes_of_request_at_retrieval\": 6, \n",
      "  \"post_was_edited\": false, \n",
      "  \"request_id\": \"t3_w5491\", \n",
      "  \"request_number_of_comments_at_retrieval\": 7, \n",
      "  \"request_text\": \"I'm not in College, or a starving artist or anything like that. I've just been a bit unlucky lately. I'm a 36 year old single guy with a job. But rent, and other bills killed me this month. I thought I had enough funds in my account to at least keep me set with Mr. Noodles, I forgot about my monthly banking fee.\\n\\nI had a small bag of chips Wednesday afternoon, and I get paid Monday, so I'll be fine then.. It's just really painful at this point and food is something I'm constantly thinking about.\\n\\nI've got a few bucks to get on the bus to work on Saturday, so I can't really use that.\\n\\nI'm really embarrassed to even be asking like this and I'm not sure how it works, so please be patient with me.\\n\\nI guess that covers it. Thank you in advance.\\n\\nCheers folks.\", \n",
      "  \"request_text_edit_aware\": \"I'm not in College, or a starving artist or anything like that. I've just been a bit unlucky lately. I'm a 36 year old single guy with a job. But rent, and other bills killed me this month. I thought I had enough funds in my account to at least keep me set with Mr. Noodles, I forgot about my monthly banking fee.\\n\\nI had a small bag of chips Wednesday afternoon, and I get paid Monday, so I'll be fine then.. It's just really painful at this point and food is something I'm constantly thinking about.\\n\\nI've got a few bucks to get on the bus to work on Saturday, so I can't really use that.\\n\\nI'm really embarrassed to even be asking like this and I'm not sure how it works, so please be patient with me.\\n\\nI guess that covers it. Thank you in advance.\\n\\nCheers folks.\", \n",
      "  \"request_title\": \"[Request] Ontario, Canada - On my 3rd of 5 days without food, and it's getting unbearable. Can anyone help?\", \n",
      "  \"requester_account_age_in_days_at_request\": 14.416875, \n",
      "  \"requester_account_age_in_days_at_retrieval\": 531.9697222222222, \n",
      "  \"requester_days_since_first_post_on_raop_at_request\": 0.0, \n",
      "  \"requester_days_since_first_post_on_raop_at_retrieval\": 517.5111805555556, \n",
      "  \"requester_number_of_comments_at_request\": 8, \n",
      "  \"requester_number_of_comments_at_retrieval\": 93, \n",
      "  \"requester_number_of_comments_in_raop_at_request\": 0, \n",
      "  \"requester_number_of_comments_in_raop_at_retrieval\": 4, \n",
      "  \"requester_number_of_posts_at_request\": 1, \n",
      "  \"requester_number_of_posts_at_retrieval\": 6, \n",
      "  \"requester_number_of_posts_on_raop_at_request\": 0, \n",
      "  \"requester_number_of_posts_on_raop_at_retrieval\": 2, \n",
      "  \"requester_number_of_subreddits_at_request\": 8, \n",
      "  \"requester_received_pizza\": true, \n",
      "  \"requester_subreddits_at_request\": [\n",
      "    \"AdviceAnimals\", \n",
      "    \"WTF\", \n",
      "    \"funny\", \n",
      "    \"gaming\", \n",
      "    \"movies\", \n",
      "    \"technology\", \n",
      "    \"todayilearned\", \n",
      "    \"videos\"\n",
      "  ], \n",
      "  \"requester_upvotes_minus_downvotes_at_request\": 32, \n",
      "  \"requester_upvotes_minus_downvotes_at_retrieval\": 212, \n",
      "  \"requester_upvotes_plus_downvotes_at_request\": 48, \n",
      "  \"requester_upvotes_plus_downvotes_at_retrieval\": 610, \n",
      "  \"requester_user_flair\": \"shroom\", \n",
      "  \"requester_username\": \"RitalinYourMemory\", \n",
      "  \"unix_timestamp_of_request\": 1341604684.0, \n",
      "  \"unix_timestamp_of_request_utc\": 1341601084.0\n",
      "}\n",
      "The average success rate is: 24.63%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import codecs\n",
    "import json\n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "def read_dataset(path):\n",
    "  with codecs.open(path, 'r', 'utf-8') as myFile:\n",
    "    content = myFile.read()\n",
    "  dataset = json.loads(content)\n",
    "  return dataset\n",
    "\n",
    "path = './pizza_request_dataset.json'\n",
    "dataset = read_dataset(path)\n",
    "\n",
    "print 'The dataset contains %d samples.' %(len(dataset))\n",
    "print 'Available attributes: ', sorted(dataset[0].keys())\n",
    "print 'First post:'\n",
    "print json.dumps(dataset[0], sort_keys=True, indent=2)\n",
    "\n",
    "successes = [r['requester_received_pizza'] for r in dataset]\n",
    "success_rate = 100.0 * sum(successes) / float(len(successes))\n",
    "print 'The average success rate is: %.2f%%' %(success_rate)\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3969, 12233)\n",
      "(1701, 12233)\n"
     ]
    }
   ],
   "source": [
    "#read train data\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in range(int(len(shuffle) * 0.7)):\n",
    "    train.append(dataset[shuffle[i]])\n",
    "\n",
    "for i in range(int(len(shuffle) * 0.3)):\n",
    "    test.append(dataset[shuffle[len(shuffle) - i - 1]])\n",
    "    \n",
    "train_labels = np.zeros(len(train))\n",
    "test_labels = np.zeros(len(test))\n",
    "\n",
    "train_data_text = []\n",
    "test_data_text = []\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_labels[i] = train[i].get('requester_received_pizza')\n",
    "    train_data_text.append(train[i].get('request_text_edit_aware'))\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    test_labels[i] = test[i].get('requester_received_pizza')\n",
    "    test_data_text.append(test[i].get('request_text_edit_aware'))\n",
    "\n",
    "vec_train = CountVectorizer()\n",
    "tokenized_train_data = vec_train.fit_transform(train_data_text)\n",
    "\n",
    "vec_test = CountVectorizer(vocabulary=vec_train.vocabulary_)\n",
    "tokenized_test_data = vec_test.fit_transform(test_data_text)\n",
    "\n",
    "print tokenized_train_data.shape\n",
    "print tokenized_test_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.85      0.81      1287\n",
      "        1.0       0.35      0.25      0.29       414\n",
      "\n",
      "avg / total       0.68      0.70      0.69      1701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR_train = LogisticRegression()\n",
    "LR_train.fit(tokenized_train_data,train_labels)\n",
    "\n",
    "test_pred = LR_train.predict(tokenized_test_data)\n",
    "\n",
    "print(classification_report(test_labels, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding requester account age in days:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.85      0.82      1287\n",
      "        1.0       0.36      0.26      0.30       414\n",
      "\n",
      "avg / total       0.68      0.71      0.69      1701\n",
      "\n",
      "Adding requester days since first post:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.85      0.82      1287\n",
      "        1.0       0.37      0.27      0.31       414\n",
      "\n",
      "avg / total       0.68      0.71      0.69      1701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#see performance with \"requester_account_age_in_days_at_request\" and \n",
    "#\"requester_days_since_first_post_on_raop_at_request\"\n",
    "\n",
    "#append the account age in days to data\n",
    "\n",
    "train_token_mat = tokenized_train_data.todense()\n",
    "test_token_mat = tokenized_test_data.todense()\n",
    "\n",
    "train_tmp = np.zeros(len(train))\n",
    "test_tmp = np.zeros(len(test))\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_tmp[i] = train[i].get('requester_account_age_in_days_at_request')\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    test_tmp[i] = test[i].get('requester_account_age_in_days_at_request')\n",
    "\n",
    "\n",
    "train_token_mat_plus = np.c_[train_token_mat,train_tmp]\n",
    "test_token_mat_plus = np.c_[test_token_mat,test_tmp]\n",
    "\n",
    "LR_train_plus = LogisticRegression()\n",
    "LR_train_plus.fit(train_token_mat_plus,train_labels)\n",
    "\n",
    "test_pred_plus = LR_train_plus.predict(test_token_mat_plus)\n",
    "\n",
    "print \"Adding requester account age in days:\\n\"\n",
    "print(classification_report(test_labels, test_pred_plus))\n",
    "\n",
    "\n",
    "#append the requester_days_since_first_post_on_raop_at_request\n",
    "\n",
    "train_tmp = np.zeros(len(train))\n",
    "test_tmp = np.zeros(len(test))\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_tmp[i] = train[i].get('requester_days_since_first_post_on_raop_at_request')\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    test_tmp[i] = test[i].get('requester_days_since_first_post_on_raop_at_request')\n",
    "\n",
    "\n",
    "train_token_mat_plus_1 = np.c_[train_token_mat_plus,train_tmp]\n",
    "test_token_mat_plus_1 = np.c_[test_token_mat_plus,test_tmp]\n",
    "\n",
    "LR_train_plus_1 = LogisticRegression()\n",
    "LR_train_plus_1.fit(train_token_mat_plus_1,train_labels)\n",
    "\n",
    "test_pred_plus_1 = LR_train_plus_1.predict(test_token_mat_plus_1)\n",
    "\n",
    "print \"Adding requester days since first post:\\n\"\n",
    "print(classification_report(test_labels, test_pred_plus_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding requester upvotes - downvotes:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.86      0.82      1287\n",
      "        1.0       0.38      0.26      0.31       414\n",
      "\n",
      "avg / total       0.68      0.72      0.70      1701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#append requester_upvotes_minus_downvotes_at_request (did best)\n",
    "\n",
    "train_tmp = np.zeros(len(train))\n",
    "test_tmp = np.zeros(len(test))\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_tmp[i] = train[i].get('requester_upvotes_minus_downvotes_at_request')\n",
    "        \n",
    "for i in range(len(test_labels)):\n",
    "    test_tmp[i] = test[i].get('requester_upvotes_minus_downvotes_at_request')\n",
    "        \n",
    "#print max_val\n",
    "\n",
    "train_token_mat_plus_2 = np.c_[train_token_mat_plus_1,train_tmp]\n",
    "test_token_mat_plus_2 = np.c_[test_token_mat_plus_1,test_tmp]\n",
    "\n",
    "LR_train_plus_2 = LogisticRegression()\n",
    "LR_train_plus_2.fit(train_token_mat_plus_2,train_labels)\n",
    "\n",
    "test_pred_plus_2 = LR_train_plus_2.predict(test_token_mat_plus_2)\n",
    "\n",
    "print \"Adding requester upvotes - downvotes:\\n\"\n",
    "\n",
    "print(classification_report(test_labels, test_pred_plus_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding comments in RAOP\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.86      0.82      1287\n",
      "        1.0       0.38      0.27      0.31       414\n",
      "\n",
      "avg / total       0.69      0.71      0.70      1701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#append requester_number_of_comments_in_raop_at_request\n",
    "\n",
    "train_tmp = np.zeros(len(train))\n",
    "test_tmp = np.zeros(len(test))\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_tmp[i] = train[i].get('requester_number_of_comments_in_raop_at_request')\n",
    "        \n",
    "for i in range(len(test_labels)):\n",
    "    test_tmp[i] = test[i].get('requester_number_of_comments_in_raop_at_request')\n",
    "        \n",
    "#print max_val\n",
    "\n",
    "train_token_mat_plus_3 = np.c_[train_token_mat_plus_2,train_tmp]\n",
    "test_token_mat_plus_3 = np.c_[test_token_mat_plus_2,test_tmp]\n",
    "\n",
    "LR_train_plus_3 = LogisticRegression()\n",
    "LR_train_plus_3.fit(train_token_mat_plus_3,train_labels)\n",
    "\n",
    "test_pred_plus_3 = LR_train_plus_3.predict(test_token_mat_plus_3)\n",
    "\n",
    "print \"Adding comments in RAOP\\n\"\n",
    "\n",
    "print(classification_report(test_labels, test_pred_plus_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding number of subreddits:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      1.00      0.86      1287\n",
      "        1.0       0.00      0.00      0.00       414\n",
      "\n",
      "avg / total       0.57      0.76      0.65      1701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#append requester_number_of_subreddits_at_request\n",
    "\n",
    "train_tmp = np.zeros(len(train))\n",
    "test_tmp = np.zeros(len(test))\n",
    "\n",
    "max_val = 0\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_tmp[i] = train[i].get('requester_number_of_subreddits_at_request') / 235\n",
    "    \n",
    "    if max_val < train_tmp[i]:\n",
    "        max_val = train_tmp[i]\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    test_tmp[i] = test[i].get('requester_number_of_subreddits_at_request') / 235\n",
    "\n",
    "    if max_val < test_tmp[i]:\n",
    "        max_val = test_tmp[i]\n",
    "\n",
    "#print max_val\n",
    "\n",
    "train_token_mat_plus_4 = np.c_[train_token_mat_plus_3,train_tmp]\n",
    "test_token_mat_plus_4 = np.c_[test_token_mat_plus_3,test_tmp]\n",
    "\n",
    "LR_train_plus_4 = LogisticRegression()\n",
    "LR_train_plus_4.fit(train_token_mat_plus_4,train_labels)\n",
    "\n",
    "test_pred_plus_4 = LR_train_plus_4.predict(test_token_mat_plus_4)\n",
    "\n",
    "print \"Adding number of subreddits:\\n\"\n",
    "\n",
    "print(classification_report(test_labels, test_pred_plus_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.94      0.85      1287\n",
      "        1.0       0.41      0.13      0.19       414\n",
      "\n",
      "avg / total       0.68      0.74      0.69      1701\n",
      "\n",
      "Adding title NLP\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.85      0.82      1287\n",
      "        1.0       0.36      0.26      0.30       414\n",
      "\n",
      "avg / total       0.68      0.71      0.69      1701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#title NLP \n",
    "\n",
    "train_data_text_title = []\n",
    "test_data_text_title = []\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_data_text_title.append(train[i].get('request_title'))\n",
    "    #train_data_text_title.append(\"blarg\")\n",
    "    \n",
    "for i in range(len(test_labels)):\n",
    "    test_data_text_title.append(test[i].get('request_title'))\n",
    "    #test_data_text_title.append(\"blarg too\")\n",
    "    \n",
    "vec_train_title = CountVectorizer()\n",
    "tokenized_train_data_title = vec_train_title.fit_transform(train_data_text_title)\n",
    "\n",
    "vec_test_title = CountVectorizer(vocabulary=vec_train_title.vocabulary_)\n",
    "tokenized_test_data_title = vec_test_title.fit_transform(test_data_text_title)\n",
    "\n",
    "LR_train_title = LogisticRegression()\n",
    "LR_train_title.fit(tokenized_train_data_title,train_labels)\n",
    "\n",
    "test_pred_title = LR_train_title.predict(tokenized_test_data_title)\n",
    "\n",
    "print(classification_report(test_labels, test_pred_title))\n",
    "\n",
    "\n",
    "train_token_mat_title = tokenized_train_data_title.todense()\n",
    "test_token_mat_title = tokenized_test_data_title.todense()\n",
    "\n",
    "train_token_mat_plus_3 = np.c_[train_token_mat_plus_2,train_token_mat_title]\n",
    "test_token_mat_plus_3 = np.c_[test_token_mat_plus_2,test_token_mat_title]\n",
    "\n",
    "LR_train_plus_3 = LogisticRegression()\n",
    "LR_train_plus_3.fit(train_token_mat_plus_3,train_labels)\n",
    "\n",
    "test_pred_plus_3 = LR_train_plus_3.predict(test_token_mat_plus_3)\n",
    "\n",
    "print \"Adding title NLP\\n\"\n",
    "\n",
    "print(classification_report(test_labels, test_pred_plus_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#check user flair\n",
    "\n",
    "train_tmp = np.zeros((len(train),3))\n",
    "test_tmp = np.zeros((len(test),3))\n",
    "\n",
    "#encode into 3 dim array\n",
    "for i in range(len(train_labels)):\n",
    "    if train[i].get('requester_user_flair') == None:\n",
    "        train_tmp[i,0] = 1\n",
    "    if train[i].get('requester_user_flair') == \"shroom\":\n",
    "        train_tmp[i,1] = 1\n",
    "    if train[i].get('requester_user_flair') == \"PIF\":\n",
    "        train_tmp[i,2] = 1\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if test[i].get('requester_user_flair') == None:\n",
    "        test_tmp[i,0] = 1\n",
    "    if test[i].get('requester_user_flair') == \"shroom\":\n",
    "        test_tmp[i,1] = 1\n",
    "    if test[i].get('requester_user_flair') == \"PIF\":\n",
    "        test_tmp[i,2] = 1\n",
    "        \n",
    "#append flair vector\n",
    "\n",
    "train_token_mat_plus_2 = np.c_[train_token_mat_plus_1,train_tmp]\n",
    "test_token_mat_plus_2 = np.c_[test_token_mat_plus_1,test_tmp]\n",
    "\n",
    "LR_train_plus_2 = LogisticRegression()\n",
    "LR_train_plus_2.fit(train_token_mat_plus_2,train_labels)\n",
    "\n",
    "test_pred_plus_2 = LR_train_plus_2.predict(test_token_mat_plus_2)\n",
    "\n",
    "print \"Adding requester days since first post:\\n\"\n",
    "print(classification_report(test_labels, test_pred_plus_2))\n",
    "\n",
    "#print np.corrcoef(train_labels,train_tmp_0[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#append requester_upvotes_plus_downvotes_at_request\n",
    "\n",
    "train_tmp = np.zeros(len(train))\n",
    "test_tmp = np.zeros(len(test))\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_tmp[i] = train[i].get('requester_upvotes_plus_downvotes_at_request')\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    test_tmp[i] = test[i].get('requester_upvotes_plus_downvotes_at_request')\n",
    "\n",
    "\n",
    "train_token_mat_plus_3 = np.c_[train_token_mat_plus_2,train_tmp]\n",
    "test_token_mat_plus_3 = np.c_[test_token_mat_plus_2,test_tmp]\n",
    "\n",
    "LR_train_plus_3 = LogisticRegression()\n",
    "LR_train_plus_3.fit(train_token_mat_plus_3,train_labels)\n",
    "\n",
    "test_pred_plus_3 = LR_train_plus_3.predict(test_token_mat_plus_3)\n",
    "\n",
    "print \"Adding requester upvotes + downvotes:\\n\"\n",
    "\n",
    "print(classification_report(test_labels, test_pred_plus_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#append unix_timestamp_of_request\n",
    "\n",
    "train_tmp = np.zeros(len(train))\n",
    "test_tmp = np.zeros(len(test))\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    train_tmp[i] = train[i].get('unix_timestamp_of_request')\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    test_tmp[i] = test[i].get('unix_timestamp_of_request')\n",
    "\n",
    "\n",
    "train_token_mat_plus_3 = np.c_[train_token_mat_plus_2,train_tmp]\n",
    "test_token_mat_plus_3 = np.c_[test_token_mat_plus_2,test_tmp]\n",
    "\n",
    "LR_train_plus_3 = LogisticRegression()\n",
    "LR_train_plus_3.fit(train_token_mat_plus_3,train_labels)\n",
    "\n",
    "test_pred_plus_3 = LR_train_plus_3.predict(test_token_mat_plus_3)\n",
    "\n",
    "print \"Adding unix timestamp:\\n\"\n",
    "\n",
    "print(classification_report(test_labels, test_pred_plus_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create predictions for Kaggle\n",
    "test_data = pd.read_json('data/test.json')\n",
    "test_data_text = test_data.request_text_edit_aware\n",
    "\n",
    "tokenized_test_data = vec_test.fit_transform(test_data_text)\n",
    "\n",
    "test_pred = LR_train.predict(tokenized_test_data)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "predictions['request_id'] = test_data.request_id\n",
    "predictions['requester_received_pizza'] = test_pred\n",
    "predictions = predictions.set_index('request_id')\n",
    "\n",
    "predictions.to_csv('NLP_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([                           u'giver_username_if_known',\n",
      "                                               u'request_id',\n",
      "                                  u'request_text_edit_aware',\n",
      "                                            u'request_title',\n",
      "                 u'requester_account_age_in_days_at_request',\n",
      "       u'requester_days_since_first_post_on_raop_at_request',\n",
      "                  u'requester_number_of_comments_at_request',\n",
      "          u'requester_number_of_comments_in_raop_at_request',\n",
      "                     u'requester_number_of_posts_at_request',\n",
      "             u'requester_number_of_posts_on_raop_at_request',\n",
      "                u'requester_number_of_subreddits_at_request',\n",
      "                          u'requester_subreddits_at_request',\n",
      "             u'requester_upvotes_minus_downvotes_at_request',\n",
      "              u'requester_upvotes_plus_downvotes_at_request',\n",
      "                                       u'requester_username',\n",
      "                                u'unix_timestamp_of_request',\n",
      "                            u'unix_timestamp_of_request_utc'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#create predictions for Kaggle\n",
    "test_data = pd.read_json('data/test.json')\n",
    "\n",
    "print test_data.columns\n",
    "\n",
    "test_data_text = test_data.request_text_edit_aware\n",
    "\n",
    "tokenized_test_data = vec_test.fit_transform(test_data_text)\n",
    "\n",
    "test_token_mat = tokenized_test_data.todense()\n",
    "\n",
    "test_tmp = test_data.requester_account_age_in_days_at_request\n",
    "\n",
    "test_token_mat_plus = np.c_[test_token_mat,test_tmp]\n",
    "\n",
    "#append the requester_days_since_first_post_on_raop_at_request\n",
    "\n",
    "test_tmp = test_data.requester_days_since_first_post_on_raop_at_request\n",
    "\n",
    "test_token_mat_plus_1 = np.c_[test_token_mat_plus,test_tmp]\n",
    "\n",
    "test_pred = LR_train_plus_1.predict(test_token_mat_plus_1)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "predictions['request_id'] = test_data.request_id\n",
    "predictions['requester_received_pizza'] = test_pred\n",
    "predictions = predictions.set_index('request_id')\n",
    "\n",
    "predictions.to_csv('NLP_submission_v2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
